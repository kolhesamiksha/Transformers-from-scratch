{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dafa00f",
   "metadata": {},
   "source": [
    "## Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed0f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d19ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "L, d_k, d_v = 4, 8, 8           #No of words in a sequence\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77408225",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0980e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1030609393732602, 0.6858478742652863, 7.228164681456705)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why we need sqrt(d_k) in denominator\n",
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83b8b0",
   "metadata": {},
   "source": [
    "from the above variance of q and k vector the variance after applying softmax transformation is far more than matching with their multiplication so as to make the training compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6d9f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1030609393732602, 0.6858478742652863, 0.903520585182088)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bed938",
   "metadata": {},
   "source": [
    "## Masking\n",
    "This is to ensure words don't get context from words generated in the future.\n",
    "Not required in the encoders, but required int he decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0d9867a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1.],\n",
       "       [0., 0., 1., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.triu(np.ones( (L, L) ),1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a2f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask==1] = -np.infty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52d73492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48670dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92584833,        -inf,        -inf,        -inf],\n",
       "       [ 0.3325317 ,  0.53980251,        -inf,        -inf],\n",
       "       [ 0.2672791 , -1.43039679, -0.89674047,        -inf],\n",
       "       [-0.03939342, -0.92651319, -0.73519263,  1.03127194]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908128bc",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "$$\n",
    "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58374ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x) / np.sum(np.exp(x), axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5d952db",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1f3dad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.93239835e-01, -8.68848716e-01,  6.12334560e-03,\n",
       "        -9.44621979e-01, -6.01999421e-01, -9.13011694e-01,\n",
       "        -7.52002345e-01,  8.05889380e-01],\n",
       "       [ 3.91952770e-01, -1.03344328e-01,  1.02396098e+00,\n",
       "        -8.68597653e-01, -9.10805247e-01,  4.68592114e-04,\n",
       "        -1.48538129e-01,  1.25948443e-01],\n",
       "       [-6.49465346e-01, -3.50993332e-01,  5.57215007e-01,\n",
       "        -7.87876565e-01, -2.24745109e-01, -1.05654241e+00,\n",
       "        -2.45637701e-01,  5.66995708e-01],\n",
       "       [-6.65838417e-01,  3.55557769e-01, -6.50745933e-02,\n",
       "        -1.05337762e+00,  6.86674076e-02,  8.93337764e-01,\n",
       "        -3.52725324e-01,  4.36782385e-02]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ede8b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "184b83a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-1.44263697 -0.09649057 -0.77919781 -0.68726859  1.02303874 -0.05215515\n",
      "   0.98522207 -1.60586385]\n",
      " [-1.6709121  -0.45338648 -0.61241272  0.70945495  1.48492191  0.60092861\n",
      "  -0.13165875  0.28240596]\n",
      " [-1.81694942  1.17219678  1.78118943  0.59666832 -1.30004898 -1.68664413\n",
      "  -0.95071489  1.03328314]\n",
      " [ 1.81093502  0.38335629  1.25714101  0.24707683 -0.05677332 -0.9347236\n",
      "  -0.50725503 -0.42900001]]\n",
      "K\n",
      " [[-0.46027543  0.42393113  0.15914651  0.40073474  0.56760977 -0.65923352\n",
      "   1.037014   -0.47217593]\n",
      " [-0.01230478  0.50324977 -0.67668636 -1.91831969  1.07177521  1.30880129\n",
      "  -0.39288505  0.8895817 ]\n",
      " [-0.2313224  -0.58233207 -1.29892357  0.40530313 -0.29995753 -0.4368623\n",
      "   0.97749125 -0.38686029]\n",
      " [-0.08679261  0.32476516  1.28829248 -0.30865827 -1.59258497 -0.17311546\n",
      "  -1.69039193 -0.69127806]]\n",
      "V\n",
      " [[-0.89323983 -0.86884872  0.00612335 -0.94462198 -0.60199942 -0.91301169\n",
      "  -0.75200234  0.80588938]\n",
      " [ 1.43655668  0.51885726  1.8512589  -0.80680512 -1.16180247  0.74294492\n",
      "   0.34195729 -0.42670721]\n",
      " [-1.09207073  0.7974544   1.56334023 -0.27475452  1.53306155 -2.57156142\n",
      "   1.03153841  0.38463535]\n",
      " [-0.81182781  0.67667101 -0.63836521 -1.25856067  0.22194713  2.12602685\n",
      "  -0.55055072 -0.20947004]]\n",
      "New V\n",
      " [[-8.93239835e-01 -8.68848716e-01  6.12334560e-03 -9.44621979e-01\n",
      "  -6.01999421e-01 -9.13011694e-01 -7.52002345e-01  8.05889380e-01]\n",
      " [ 3.91952770e-01 -1.03344328e-01  1.02396098e+00 -8.68597653e-01\n",
      "  -9.10805247e-01  4.68592114e-04 -1.48538129e-01  1.25948443e-01]\n",
      " [-6.49465346e-01 -3.50993332e-01  5.57215007e-01 -7.87876565e-01\n",
      "  -2.24745109e-01 -1.05654241e+00 -2.45637701e-01  5.66995708e-01]\n",
      " [-6.65838417e-01  3.55557769e-01 -6.50745933e-02 -1.05337762e+00\n",
      "   6.86674076e-02  8.93337764e-01 -3.52725324e-01  4.36782385e-02]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.44836702 0.55163298 0.         0.        ]\n",
      " [0.66874547 0.12245304 0.20880148 0.        ]\n",
      " [0.20713214 0.08530535 0.10329178 0.60427073]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43ea27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78be2217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
